{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-05T15:02:00.749273Z",
     "start_time": "2025-05-05T15:02:00.717566Z"
    }
   },
   "source": [
    "from numpy.typing import NDArray, DTypeLike\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# TODO\n",
    "# Add Batch Normalization With Scale and Shift\n",
    "# Add Skip Connection\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self,\n",
    "                 X: NDArray,\n",
    "                 y: NDArray,\n",
    "                 batch_size: int = 32,\n",
    "                 shuffle: bool = True):\n",
    "        assert X.shape[0] == y.shape[0], \"The number of examples in x and y must match\"\n",
    "\n",
    "        y = y[:, np.newaxis] if y.ndim == 1 else y\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_samples = X.shape[0]\n",
    "        self.indices = np.arange(self.num_samples)\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "        for start in range(0, self.num_samples, self.batch_size):\n",
    "            end = start + self.batch_size\n",
    "            batch_idx = self.indices[start:end]\n",
    "\n",
    "            yield self.X[batch_idx], self.y[batch_idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.num_samples + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "class Module(ABC):\n",
    "    def __init__(self):\n",
    "        self._params: list[NDArray] = []\n",
    "        self._grads: list[NDArray] = []\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: NDArray) -> NDArray:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, grad: NDArray) -> NDArray:\n",
    "        pass\n",
    "\n",
    "    def parameters(self) -> list[tuple[NDArray, NDArray]]:\n",
    "        return list(zip(self._params, self._grads))\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        for grad in self._grads:\n",
    "            grad.fill(0)\n",
    "\n",
    "# Convolutional layer (single-channel inputs assumed)\n",
    "class Conv2D(Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int,\n",
    "                 kernel_size: int, stride: int = 1, padding: int = 0) -> None:\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        fan_in = in_channels * kernel_size * kernel_size\n",
    "        w = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * np.sqrt(2.0 / fan_in)\n",
    "        b = np.zeros(out_channels)\n",
    "\n",
    "        self._params = [w, b]\n",
    "        self._grads = [np.zeros_like(w), np.zeros_like(b)]\n",
    "        self._cache: tuple[NDArray, NDArray] = (np.zeros_like(w), np.zeros_like(w))\n",
    "\n",
    "    def _out_size(self, size: int) -> int:\n",
    "        return (size + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "\n",
    "    def forward(self, x: NDArray) -> NDArray:\n",
    "        # x.shape: (batch_size, in_channels, height, width)\n",
    "        # out.shape: (batch_size, out_channels, height_out, width_out)\n",
    "        # in_channels: 1 - Black/White, 3 - RGB\n",
    "        # out_channels: Number of filters\n",
    "        w, b = self._params\n",
    "        stride, padding, kernel_size = self.stride, self.padding, self.kernel_size\n",
    "        out_channels = w.shape[0]\n",
    "        batch_size, in_channels, height, width = x.shape\n",
    "        assert in_channels == w.shape[1]\n",
    "\n",
    "        # pad\n",
    "        paddings = ((0, 0), (0, 0), (padding, padding), (padding, padding))\n",
    "        x_pad = np.pad(x, paddings, mode='constant', constant_values=0)\n",
    "\n",
    "        height_out = self._out_size(height)\n",
    "        width_out = self._out_size(width)\n",
    "\n",
    "        out = np.zeros((batch_size, out_channels, height_out, width_out))\n",
    "        self._cache = (x, x_pad)\n",
    "\n",
    "        for n in range(batch_size):\n",
    "            for out_c in range(out_channels):\n",
    "                kernel = w[out_c]\n",
    "                bias = b[out_c]\n",
    "                for i in range(height_out):\n",
    "                    for j in range(width_out):\n",
    "                        #h0 - height_stride, w0 - width_stride\n",
    "                        h0, w0 = stride*i, stride*j\n",
    "                        region = x_pad[n, :, h0:h0+kernel_size, w0:w0+kernel_size]\n",
    "                        out[n, out_c, i, j] = np.sum(region * kernel) + bias\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad: NDArray) -> NDArray:\n",
    "        # x.shape: (batch_size, in_channels, height, width)\n",
    "        # grad.shape: (batch_size, out_channels, height_out, width_out)\n",
    "        x, x_pad = self._cache\n",
    "        w, b = self._params\n",
    "        dw, db = self._grads\n",
    "\n",
    "        stride, padding, kernel_size = self.stride, self.padding, self.kernel_size\n",
    "        batch_size, in_channels, height, width = x.shape\n",
    "        _, out_channels, height_out, width_out = grad.shape\n",
    "\n",
    "        dw.fill(0)\n",
    "        db.fill(0)\n",
    "\n",
    "        dx_pad = np.zeros_like(x_pad)\n",
    "\n",
    "        for n in range(batch_size):\n",
    "            for out_c in range(out_channels):\n",
    "                kernel = w[out_c]\n",
    "                for i in range(height_out):\n",
    "                    for j in range(width_out):\n",
    "                        #h0 - height_stride, w0 - width_stride\n",
    "                        h0, w0 = stride*i, stride*j\n",
    "\n",
    "                        region = x_pad[n, :, h0:h0+kernel_size, w0:w0+kernel_size]\n",
    "\n",
    "                        grad_out = grad[n, out_c, i, j]\n",
    "\n",
    "                        dw[out_c] += grad_out * region\n",
    "                        db[out_c] += grad_out\n",
    "\n",
    "                        dx_pad[n, :, h0:h0+kernel_size, w0:w0+kernel_size] += grad_out * kernel\n",
    "        # unpad\n",
    "        if padding > 0:\n",
    "            return dx_pad[:, :, padding:-padding, padding:-padding]\n",
    "        return dx_pad\n",
    "\n",
    "class Flatten(Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._cache: tuple[int, ...] = ()\n",
    "\n",
    "    def forward(self, x: NDArray) -> NDArray:\n",
    "        # x shape: (batch_size, ...)\n",
    "        self._cache = x.shape\n",
    "        batch_size = x.shape[0]\n",
    "        return x.reshape(batch_size, -1)\n",
    "\n",
    "    def backward(self, grad: NDArray) -> NDArray:\n",
    "        # grad shape: (batch_size, features)\n",
    "        return grad.reshape(self._cache)\n",
    "\n",
    "\n",
    "class MaxPool2D(Module):\n",
    "    def __init__(self, kernel_size: int, stride: int = 1) -> None:\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self._cache: tuple[NDArray, NDArray] = (np.zeros_like(1), np.zeros_like(1))  # (x, mask)\n",
    "\n",
    "    def _out_size(self, size: int) -> int:\n",
    "        return (size - self.kernel_size) // self.stride + 1\n",
    "\n",
    "    def forward(self, x: NDArray) -> np.ndarray:\n",
    "        # x shape: (batch_size, channels, height, width)\n",
    "        stride, kernel_size = self.stride, self.kernel_size\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        height_out = self._out_size(height)\n",
    "        width_out = self._out_size(width)\n",
    "\n",
    "        out = np.zeros((batch_size, channels, height_out, width_out))\n",
    "        mask = np.zeros_like(x, dtype=bool)\n",
    "\n",
    "        for n in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(height_out):\n",
    "                    for j in range(width_out):\n",
    "                        #h0 - height_stride, w0 - width_stride\n",
    "                        h0, w0 = i * stride, j * stride\n",
    "                        window = x[n, c, h0:h0 + kernel_size, w0:w0 + kernel_size]\n",
    "                        max_val = np.max(window)\n",
    "                        out[n, c, i, j] = max_val\n",
    "                        # create mask\n",
    "                        mask[n, c, h0:h0 + kernel_size, w0:w0 + kernel_size] = (window == max_val)\n",
    "\n",
    "        self._cache = (x.shape, mask)\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        # grad shape: (batch_size, channels, h_out, w_out)\n",
    "        x_shape, mask = self._cache\n",
    "        stride, kernel_size = self.stride, self.kernel_size\n",
    "        batch_size, channels, height, width = x_shape\n",
    "        _, _, height_out, width_out = grad.shape\n",
    "\n",
    "        dx = np.zeros(x_shape)\n",
    "        for n in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(height_out):\n",
    "                    for j in range(width_out):\n",
    "                        #h0 - height_stride, w0 - width_stride\n",
    "                        h0, w0 = i * stride, j * stride\n",
    "\n",
    "                        # distribute gradient to max positions\n",
    "                        dx[n, c, h0:h0 + kernel_size, w0:w0 + kernel_size] += \\\n",
    "                            mask[n, c, h0:h0 + kernel_size, w0:w0 + kernel_size] * grad[n, c, i, j]\n",
    "        return dx\n",
    "\n",
    "class Sequential(Module):\n",
    "    def __init__(self, *layers: Module):\n",
    "        super().__init__()\n",
    "        self.layers = list(layers)\n",
    "\n",
    "    def forward(self, x: NDArray) -> NDArray:\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad: NDArray) -> NDArray:\n",
    "        grad_out = grad\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_out = layer.backward(grad_out)\n",
    "        return grad_out\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.parameters())\n",
    "        return params\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for layer in self.layers:\n",
    "            layer.zero_grad()\n",
    "\n",
    "class DropOut(Module):\n",
    "    def __init__(self, p: float = 0.5):\n",
    "        super().__init__()\n",
    "        assert 0 <= p < 1, \"Dropout probability must be in [0, 1)\"\n",
    "        self.p = p\n",
    "        self.mask: NDArray = np.array([])\n",
    "\n",
    "    def forward(self, x: NDArray) -> NDArray:\n",
    "        #if self.training and self.p > 0:\n",
    "        self.mask = (np.random.rand(*x.shape) > self.p) / (1 - self.p)\n",
    "        return x * self.mask\n",
    "        #return x\n",
    "\n",
    "    def backward(self, grad: NDArray) -> NDArray:\n",
    "        #if self.training and self.p > 0:\n",
    "        return grad * self.mask\n",
    "        #return grad\n",
    "\n",
    "# Linear layer: y = Wx + b\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features: int, out_features: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        weight: NDArray = np.random.randn(out_features, in_features) * np.sqrt(2 / in_features)\n",
    "        bias: NDArray = np.zeros((out_features, 1))\n",
    "\n",
    "        self._params = [weight, bias]\n",
    "        self._grads = [np.zeros_like(weight), np.zeros_like(bias)]\n",
    "\n",
    "        self._input: NDArray = np.zeros((in_features, 1))\n",
    "\n",
    "    def forward(self, x: NDArray) -> NDArray:\n",
    "        self._input = x\n",
    "        W, b = self._params\n",
    "        return W @ x + b\n",
    "\n",
    "    # grad it's dLoss/dy\n",
    "    def backward(self, grad: NDArray) -> NDArray:\n",
    "        W, b = self._params\n",
    "        dW, db = self._grads\n",
    "        x = self._input\n",
    "\n",
    "        dW[...] = grad @ x.T\n",
    "        db[...] = np.sum(grad, axis=1, keepdims=True)\n",
    "\n",
    "        return W.T @ grad\n",
    "\n",
    "class Normalization(ABC):\n",
    "    @abstractmethod\n",
    "    def penalty(self, weights: NDArray) -> float:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def grad(self, weights: NDArray) -> NDArray:\n",
    "        pass\n",
    "\n",
    "class L1(Normalization):\n",
    "    def penalty(self, weights: NDArray) -> float:\n",
    "        return np.sum(np.abs(weights))\n",
    "\n",
    "    def grad(self, weights: NDArray) -> NDArray:\n",
    "        return np.sign(weights)\n",
    "\n",
    "class L2(Normalization):\n",
    "    def penalty(self, weights: NDArray) -> float:\n",
    "        return 0.5 * np.sum(weights ** 2)\n",
    "\n",
    "    def grad(self, weights: NDArray) -> NDArray:\n",
    "        return weights\n",
    "\n",
    "class ElasticNet(Normalization):\n",
    "    def __init__(self, ratio: float = 0.5, lambda_: float = 1e-4) -> None:\n",
    "        self.ratio = ratio\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def penalty(self, weights: NDArray) -> float:\n",
    "        return self.ratio * L1().penalty(weights) + (1 - self.ratio) * L2().penalty(weights)\n",
    "\n",
    "    def grad(self, weights: NDArray) -> NDArray:\n",
    "        return self.ratio * L1().grad(weights) + (1 - self.ratio) * L2().grad(weights)\n",
    "\n",
    "# Activation: ReLU\n",
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._cache: NDArray = np.zeros_like(1)\n",
    "\n",
    "    def forward(self, x: NDArray) -> NDArray:\n",
    "        self._cache = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, grad: NDArray) -> NDArray:\n",
    "        return grad * (self._cache > 0)\n",
    "\n",
    "# Activation: Sigmoid\n",
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._cache: NDArray = np.zeros_like(1)\n",
    "\n",
    "    def forward(self, x: NDArray) -> NDArray:\n",
    "        self._cache = 1 / (1 + np.exp(-x))\n",
    "        return self._cache\n",
    "\n",
    "    def backward(self, grad: NDArray) -> NDArray:\n",
    "        return self._cache * (1 - self._cache)\n",
    "\n",
    "class CrossEntropy:\n",
    "    def __init__(self):\n",
    "        self.probs: NDArray = np.array([])\n",
    "\n",
    "    def forward(self, logits: NDArray, labels: NDArray) -> float:\n",
    "        max_logit: DTypeLike = np.max(logits, axis=0, keepdims=True) # Avoid overflow\n",
    "        exps = np.exp(logits - max_logit)\n",
    "        self.probs = exps / np.sum(exps, axis=0, keepdims=True) # Softmax(logits)\n",
    "\n",
    "        m = labels.shape[1]\n",
    "        loss = -np.sum(labels * np.log(self.probs + 1e-15)) / m\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def backward(self, labels: NDArray) -> NDArray:\n",
    "        m = labels.shape[1]\n",
    "        return (self.probs - labels) / m\n",
    "\n",
    "class Optimizer(ABC):\n",
    "    def __init__(self, params: list[tuple[NDArray, NDArray]], lr: float = 1e-2) -> None:\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self) -> None:\n",
    "        pass\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def step(self) -> None:\n",
    "        for (param, grad) in self.params:\n",
    "            param -= self.lr * grad\n",
    "\n",
    "class Adagrad(Optimizer):\n",
    "    def __init__(self, params: list[tuple[NDArray, NDArray]],\n",
    "                 lr: float = 1e-2, eps: float = 1e-15) -> None:\n",
    "        super().__init__(params, lr)\n",
    "\n",
    "        self.eps = eps\n",
    "        self.cache: list[NDArray] = [np.zeros_like(param) for param, _ in params]\n",
    "\n",
    "    def step(self) -> None:\n",
    "        for idx, (p, grad) in enumerate(self.params):\n",
    "            self.cache[idx] += grad ** 2\n",
    "            lr = self.lr / (np.sqrt(self.cache[idx]) + self.eps)\n",
    "            p -= lr * grad\n",
    "\n",
    "class RMSProp(Optimizer):\n",
    "    def __init__(self, params: list[tuple[NDArray, NDArray]],\n",
    "                 lr: float = 1e-2, beta: float = 0.9, eps: float = 1e-15) -> None:\n",
    "        super().__init__(params, lr)\n",
    "\n",
    "        self.beta, self.eps = beta, eps\n",
    "        self.cache: list[NDArray] = [np.zeros_like(param) for param, _ in params]\n",
    "\n",
    "    def step(self) -> None:\n",
    "        for idx, (p, grad) in enumerate(self.params):\n",
    "            self.cache[idx] = self.beta * self.cache[idx] + (1 - self.beta) * grad ** 2\n",
    "            lr = self.lr / (np.sqrt(self.cache[idx]) + self.eps)\n",
    "            p -= lr * grad\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, params: list[tuple[NDArray, NDArray]], lr: float = 1e-3,\n",
    "                 beta1: float = 0.9, beta2: float = 0.999, eps: float = 1e-8):\n",
    "        super().__init__(params, lr)\n",
    "\n",
    "        self.beta1, self.beta2, self.eps = beta1, beta2, eps\n",
    "        self.m = [np.zeros_like(p) for p, _ in params]\n",
    "        self.v = [np.zeros_like(p) for p, _ in params]\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self) -> None:\n",
    "        self.t += 1\n",
    "        for idx, (p, grad) in enumerate(self.params):\n",
    "            self.m[idx] = self.beta1 * self.m[idx] + (1 - self.beta1) * grad\n",
    "            self.v[idx] = self.beta2 * self.v[idx] + (1 - self.beta2) * (grad ** 2)\n",
    "\n",
    "            m_hat = self.m[idx] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[idx] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            p -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)"
   ],
   "outputs": [],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
