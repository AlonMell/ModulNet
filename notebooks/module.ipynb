{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from numpy.typing import NDArray, DTypeLike\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# TODO\n",
    "# Add Batch Normalization With Scale and Shift\n",
    "# Add Skip Connection\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self,\n",
    "                 X: NDArray,\n",
    "                 y: NDArray,\n",
    "                 batch_size: int = 32,\n",
    "                 shuffle: bool = True):\n",
    "        assert X.shape[0] == y.shape[0], \"The number of examples in x and y must match\"\n",
    "\n",
    "        y = y[:, np.newaxis] if y.ndim == 1 else y\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_samples = X.shape[0]\n",
    "        self.indices = np.arange(self.num_samples)\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "        for start in range(0, self.num_samples, self.batch_size):\n",
    "            end = start + self.batch_size\n",
    "            batch_idx = self.indices[start:end]\n",
    "\n",
    "            yield self.X[batch_idx], self.y[batch_idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.num_samples + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "class Module(ABC):\n",
    "    def __init__(self):\n",
    "        self._params: list[NDArray] = []\n",
    "        self._grads: list[NDArray] = []\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: NDArray) -> NDArray:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, grad: NDArray) -> NDArray:\n",
    "        pass\n",
    "\n",
    "    def parameters(self) -> list[tuple[NDArray, NDArray]]:\n",
    "        return list(zip(self._params, self._grads))\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        for grad in self._grads:\n",
    "            grad.fill(0)\n",
    "\n",
    "# Convolutional layer (single-channel inputs assumed)\n",
    "class Conv2D(Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int,\n",
    "                 kernel_size: int, stride: int = 1, padding: int = 0) -> None:\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        fan_in = in_channels * kernel_size * kernel_size\n",
    "        w = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * np.sqrt(2.0 / fan_in)\n",
    "        b = np.zeros(out_channels)\n",
    "\n",
    "        self._params = [w, b]\n",
    "        self._grads = [np.zeros_like(w), np.zeros_like(b)]\n",
    "        self._cache: tuple[NDArray, NDArray] = (np.zeros_like(w), np.zeros_like(w))\n",
    "\n",
    "    def _out_size(self, size: int) -> int:\n",
    "        return (size + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "\n",
    "    def forward(self, x: NDArray) -> NDArray:\n",
    "        # x.shape: (batch_size, in_channels, height, width)\n",
    "        # out.shape: (batch_size, out_channels, height_out, width_out)\n",
    "        # in_channels: 1 - Black/White, 3 - RGB\n",
    "        # out_channels: Number of filters\n",
    "        w, b = self._params\n",
    "        stride, padding, kernel_size = self.stride, self.padding, self.kernel_size\n",
    "        out_channels = w.shape[0]\n",
    "        batch_size, in_channels, height, width = x.shape\n",
    "        assert in_channels == w.shape[1]\n",
    "\n",
    "        # pad\n",
    "        paddings = ((0, 0), (0, 0), (padding, padding), (padding, padding))\n",
    "        x_pad = np.pad(x, paddings, mode='constant', constant_values=0)\n",
    "\n",
    "        height_out = self._out_size(height)\n",
    "        width_out = self._out_size(width)\n",
    "\n",
    "        out = np.zeros((batch_size, out_channels, height_out, width_out))\n",
    "        self._cache = (x, x_pad)\n",
    "\n",
    "        for n in range(batch_size):\n",
    "            for out_c in range(out_channels):\n",
    "                kernel = w[out_c]\n",
    "                bias = b[out_c]\n",
    "                for i in range(height_out):\n",
    "                    for j in range(width_out):\n",
    "                        #h0 - height_stride, w0 - width_stride\n",
    "                        h0, w0 = stride*i, stride*j\n",
    "                        region = x_pad[n, :, h0:h0+kernel_size, w0:w0+kernel_size]\n",
    "                        out[n, out_c, i, j] = np.sum(region * kernel) + bias\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad: NDArray) -> NDArray:\n",
    "        # x.shape: (batch_size, in_channels, height, width)\n",
    "        # grad.shape: (batch_size, out_channels, height_out, width_out)\n",
    "        x, x_pad = self._cache\n",
    "        w, b = self._params\n",
    "        dw, db = self._grads\n",
    "\n",
    "        stride, padding, kernel_size = self.stride, self.padding, self.kernel_size\n",
    "        batch_size, in_channels, height, width = x.shape\n",
    "        _, out_channels, height_out, width_out = grad.shape\n",
    "\n",
    "        dw.fill(0)\n",
    "        db.fill(0)\n",
    "\n",
    "        dx_pad = np.zeros_like(x_pad)\n",
    "\n",
    "        for n in range(batch_size):\n",
    "            for out_c in range(out_channels):\n",
    "                kernel = w[out_c]\n",
    "                for i in range(height_out):\n",
    "                    for j in range(width_out):\n",
    "                        #h0 - height_stride, w0 - width_stride\n",
    "                        h0, w0 = stride*i, stride*j\n",
    "\n",
    "                        region = x_pad[n, :, h0:h0+kernel_size, w0:w0+kernel_size]\n",
    "\n",
    "                        grad_out = grad[n, out_c, i, j]\n",
    "\n",
    "                        dw[out_c] += grad_out * region\n",
    "                        db[out_c] += grad_out\n",
    "\n",
    "                        dx_pad[n, :, h0:h0+kernel_size, w0:w0+kernel_size] += grad_out * kernel\n",
    "        # unpad\n",
    "        if padding > 0:\n",
    "            return dx_pad[:, :, padding:-padding, padding:-padding]\n",
    "        return dx_pad\n",
    "\n",
    "class Flatten(Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._cache: tuple[int, ...] = ()\n",
    "\n",
    "    def forward(self, x: NDArray) -> NDArray:\n",
    "        # x shape: (batch_size, ...)\n",
    "        self._cache = x.shape\n",
    "        batch_size = x.shape[0]\n",
    "        return x.reshape(batch_size, -1)\n",
    "\n",
    "    def backward(self, grad: NDArray) -> NDArray:\n",
    "        # grad shape: (batch_size, features)\n",
    "        return grad.reshape(self._cache)\n",
    "\n",
    "\n",
    "class MaxPool2D(Module):\n",
    "    def __init__(self, kernel_size: int, stride: int = 1) -> None:\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self._cache: tuple[NDArray, NDArray] = (np.zeros_like(1), np.zeros_like(1))  # (x, mask)\n",
    "\n",
    "    def _out_size(self, size: int) -> int:\n",
    "        return (size - self.kernel_size) // self.stride + 1\n",
    "\n",
    "    def forward(self, x: NDArray) -> np.ndarray:\n",
    "        # x shape: (batch_size, channels, height, width)\n",
    "        stride, kernel_size = self.stride, self.kernel_size\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        height_out = self._out_size(height)\n",
    "        width_out = self._out_size(width)\n",
    "\n",
    "        out = np.zeros((batch_size, channels, height_out, width_out))\n",
    "        mask = np.zeros_like(x, dtype=bool)\n",
    "\n",
    "        for n in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(height_out):\n",
    "                    for j in range(width_out):\n",
    "                        #h0 - height_stride, w0 - width_stride\n",
    "                        h0, w0 = i * stride, j * stride\n",
    "                        window = x[n, c, h0:h0 + kernel_size, w0:w0 + kernel_size]\n",
    "                        max_val = np.max(window)\n",
    "                        out[n, c, i, j] = max_val\n",
    "                        # create mask\n",
    "                        mask[n, c, h0:h0 + kernel_size, w0:w0 + kernel_size] = (window == max_val)\n",
    "\n",
    "        self._cache = (x.shape, mask)\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        # grad shape: (batch_size, channels, h_out, w_out)\n",
    "        x_shape, mask = self._cache\n",
    "        stride, kernel_size = self.stride, self.kernel_size\n",
    "        batch_size, channels, height, width = x_shape\n",
    "        _, _, height_out, width_out = grad.shape\n",
    "\n",
    "        dx = np.zeros(x_shape)\n",
    "        for n in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(height_out):\n",
    "                    for j in range(width_out):\n",
    "                        #h0 - height_stride, w0 - width_stride\n",
    "                        h0, w0 = i * stride, j * stride\n",
    "\n",
    "                        # distribute gradient to max positions\n",
    "                        dx[n, c, h0:h0 + kernel_size, w0:w0 + kernel_size] += \\\n",
    "                            mask[n, c, h0:h0 + kernel_size, w0:w0 + kernel_size] * grad[n, c, i, j]\n",
    "        return dx\n",
    "\n",
    "class Sequential(Module):\n",
    "    def __init__(self, *layers: Module):\n",
    "        super().__init__()\n",
    "        self.layers = list(layers)\n",
    "\n",
    "    def forward(self, x: NDArray) -> NDArray:\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad: NDArray) -> NDArray:\n",
    "        grad_out = grad\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_out = layer.backward(grad_out)\n",
    "        return grad_out\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.parameters())\n",
    "        return params\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for layer in self.layers:\n",
    "            layer.zero_grad()\n",
    "\n",
    "class DropOut(Module):\n",
    "    def __init__(self, p: float = 0.5):\n",
    "        super().__init__()\n",
    "        assert 0 <= p < 1, \"Dropout probability must be in [0, 1)\"\n",
    "        self.p = p\n",
    "        self.mask: NDArray = np.array([])\n",
    "\n",
    "    def forward(self, x: NDArray) -> NDArray:\n",
    "        #if self.training and self.p > 0:\n",
    "        self.mask = (np.random.rand(*x.shape) > self.p) / (1 - self.p)\n",
    "        return x * self.mask\n",
    "        #return x\n",
    "\n",
    "    def backward(self, grad: NDArray) -> NDArray:\n",
    "        #if self.training and self.p > 0:\n",
    "        return grad * self.mask\n",
    "        #return grad\n",
    "\n",
    "# Linear layer: y = x * W^T + b\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features: int, out_features: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        weight: NDArray = np.random.randn(out_features, in_features) * np.sqrt(\n",
    "            2 / in_features\n",
    "        )\n",
    "        bias: NDArray = np.zeros(out_features)\n",
    "\n",
    "        self._params = [weight, bias]\n",
    "        self._grads = [np.zeros_like(weight), np.zeros_like(bias)]\n",
    "\n",
    "        self._input: NDArray = np.zeros(in_features)\n",
    "\n",
    "    def forward(self, x: NDArray) -> NDArray:\n",
    "        # x.shape: (batch_size, in_features)\n",
    "        self._input = x\n",
    "        W, b = self._params\n",
    "        # (batch, in) · (in, out) = (batch, out)\n",
    "        return x @ W.T + b\n",
    "\n",
    "    # grad it's dLoss/dy\n",
    "    def backward(self, grad: NDArray) -> NDArray:\n",
    "        # grad: (batch_size, out_features)\n",
    "        W, b = self._params\n",
    "        dW, db = self._grads\n",
    "        x = self._input\n",
    "\n",
    "        # dW: (out, in) = grad^T (out×batch) @ x (batch×in)\n",
    "        dW[...] = grad.T @ x\n",
    "        db[...] = np.sum(grad, axis=0, keepdims=True)\n",
    "\n",
    "        return grad @ W\n",
    "\n",
    "class Normalization(ABC):\n",
    "    @abstractmethod\n",
    "    def penalty(self, weights: NDArray) -> float:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def grad(self, weights: NDArray) -> NDArray:\n",
    "        pass\n",
    "\n",
    "class L1(Normalization):\n",
    "    def penalty(self, weights: NDArray) -> float:\n",
    "        return np.sum(np.abs(weights))\n",
    "\n",
    "    def grad(self, weights: NDArray) -> NDArray:\n",
    "        return np.sign(weights)\n",
    "\n",
    "class L2(Normalization):\n",
    "    def penalty(self, weights: NDArray) -> float:\n",
    "        return 0.5 * np.sum(weights ** 2)\n",
    "\n",
    "    def grad(self, weights: NDArray) -> NDArray:\n",
    "        return weights\n",
    "\n",
    "class ElasticNet(Normalization):\n",
    "    def __init__(self, ratio: float = 0.5, lambda_: float = 1e-4) -> None:\n",
    "        self.ratio = ratio\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def penalty(self, weights: NDArray) -> float:\n",
    "        return self.ratio * L1().penalty(weights) + (1 - self.ratio) * L2().penalty(weights)\n",
    "\n",
    "    def grad(self, weights: NDArray) -> NDArray:\n",
    "        return self.ratio * L1().grad(weights) + (1 - self.ratio) * L2().grad(weights)\n",
    "\n",
    "# Activation: ReLU\n",
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._cache: NDArray = np.zeros_like(1)\n",
    "\n",
    "    def forward(self, x: NDArray) -> NDArray:\n",
    "        self._cache = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, grad: NDArray) -> NDArray:\n",
    "        return grad * (self._cache > 0)\n",
    "\n",
    "# Activation: Sigmoid\n",
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._cache: NDArray = np.zeros_like(1)\n",
    "\n",
    "    def forward(self, x: NDArray) -> NDArray:\n",
    "        self._cache = 1 / (1 + np.exp(-x))\n",
    "        return self._cache\n",
    "\n",
    "    def backward(self, grad: NDArray) -> NDArray:\n",
    "        return self._cache * (1 - self._cache)\n",
    "\n",
    "class CrossEntropy:\n",
    "    def __init__(self):\n",
    "        self.probs: NDArray = np.array([])\n",
    "\n",
    "    def forward(self, logits: NDArray, labels: NDArray) -> float:\n",
    "        max_logit: DTypeLike = np.max(logits, axis=1, keepdims=True)  # Avoid overflow\n",
    "        exps = np.exp(logits - max_logit)\n",
    "        self.probs = exps / np.sum(exps, axis=1, keepdims=True)  # Softmax(logits)\n",
    "\n",
    "        m = labels.shape[0]\n",
    "        loss = -np.sum(labels * np.log(self.probs + 1e-15)) / m\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def backward(self, labels: NDArray) -> NDArray:\n",
    "        m = labels.shape[0]\n",
    "        return (self.probs - labels) / m\n",
    "\n",
    "class Optimizer(ABC):\n",
    "    def __init__(self, params: list[tuple[NDArray, NDArray]], lr: float = 1e-2) -> None:\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self) -> None:\n",
    "        pass\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def step(self) -> None:\n",
    "        for (param, grad) in self.params:\n",
    "            param -= self.lr * grad\n",
    "\n",
    "class Adagrad(Optimizer):\n",
    "    def __init__(self, params: list[tuple[NDArray, NDArray]],\n",
    "                 lr: float = 1e-2, eps: float = 1e-15) -> None:\n",
    "        super().__init__(params, lr)\n",
    "\n",
    "        self.eps = eps\n",
    "        self.cache: list[NDArray] = [np.zeros_like(param) for param, _ in params]\n",
    "\n",
    "    def step(self) -> None:\n",
    "        for idx, (p, grad) in enumerate(self.params):\n",
    "            self.cache[idx] += grad ** 2\n",
    "            lr = self.lr / (np.sqrt(self.cache[idx]) + self.eps)\n",
    "            p -= lr * grad\n",
    "\n",
    "class RMSProp(Optimizer):\n",
    "    def __init__(self, params: list[tuple[NDArray, NDArray]],\n",
    "                 lr: float = 1e-2, beta: float = 0.9, eps: float = 1e-15) -> None:\n",
    "        super().__init__(params, lr)\n",
    "\n",
    "        self.beta, self.eps = beta, eps\n",
    "        self.cache: list[NDArray] = [np.zeros_like(param) for param, _ in params]\n",
    "\n",
    "    def step(self) -> None:\n",
    "        for idx, (p, grad) in enumerate(self.params):\n",
    "            self.cache[idx] = self.beta * self.cache[idx] + (1 - self.beta) * grad ** 2\n",
    "            lr = self.lr / (np.sqrt(self.cache[idx]) + self.eps)\n",
    "            p -= lr * grad\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, params: list[tuple[NDArray, NDArray]], lr: float = 1e-3,\n",
    "                 beta1: float = 0.9, beta2: float = 0.999, eps: float = 1e-8):\n",
    "        super().__init__(params, lr)\n",
    "\n",
    "        self.beta1, self.beta2, self.eps = beta1, beta2, eps\n",
    "        self.m = [np.zeros_like(p) for p, _ in params]\n",
    "        self.v = [np.zeros_like(p) for p, _ in params]\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self) -> None:\n",
    "        self.t += 1\n",
    "        for idx, (p, grad) in enumerate(self.params):\n",
    "            self.m[idx] = self.beta1 * self.m[idx] + (1 - self.beta1) * grad\n",
    "            self.v[idx] = self.beta2 * self.v[idx] + (1 - self.beta2) * (grad ** 2)\n",
    "\n",
    "            m_hat = self.m[idx] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[idx] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            p -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                      download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                     download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = tuple(str(i) for i in range(10))"
   ],
   "id": "f8e771ac65ad0472",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(trainloader.dataset.data.shape)\n",
    "print(testloader.dataset.data.shape)"
   ],
   "id": "7fdf197445736287",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for input, label in trainloader:\n",
    "    print(f\"input shape: {input.shape}\")\n",
    "    print(f\"label shape: {label.shape}\")\n",
    "    break"
   ],
   "id": "66f8cbc5a3f2249a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ConvNet(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = Sequential(\n",
    "            Conv2D(in_channels=1, out_channels=6, kernel_size=5), ReLU(),\n",
    "            MaxPool2D(kernel_size=2, stride=2),\n",
    "            Conv2D(in_channels=6, out_channels=16, kernel_size=5), ReLU(),\n",
    "            MaxPool2D(kernel_size=2, stride=2),\n",
    "            Flatten(),\n",
    "            Linear(16 * 4 * 4, 120), ReLU(),\n",
    "            Linear(120, 84), ReLU(),\n",
    "            Linear(84, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: NDArray) -> NDArray:\n",
    "        return self.net.forward(x)\n",
    "\n",
    "    def backward(self, x: NDArray) -> NDArray:\n",
    "        return self.net.backward(x)\n",
    "\n",
    "    def parameters(self) -> list[tuple[NDArray, NDArray]]:\n",
    "        return self.net.parameters()\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        self.net.zero_grad()"
   ],
   "id": "ece2eb41ccedce43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = ConvNet()\n",
    "loss_fn = CrossEntropy()\n",
    "optimizer = Adam(model.parameters())\n",
    "losses = []"
   ],
   "id": "5917437cee83de62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T21:00:46.831204Z",
     "start_time": "2025-05-06T20:59:12.872964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in trange(num_epochs, desc=\"Epochs\"):\n",
    "    epoch_loss = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    for X_batch, y_batch in tqdm(trainloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
    "        # X_batch: (batch, 1, 28, 28)\n",
    "        # y_batch: (batch,)\n",
    "        X_cpu = X_batch.numpy()\n",
    "        X = np.asarray(X_cpu)\n",
    "        y_idx = y_batch.numpy().astype(int)\n",
    "        y = np.eye(10, dtype=np.float32)[y_idx]\n",
    "\n",
    "        logits = model.forward(X)\n",
    "        loss = loss_fn.forward(logits, y)\n",
    "\n",
    "        grad = loss_fn.backward(y)\n",
    "\n",
    "        model.zero_grad()\n",
    "        model.backward(grad)\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = X.shape[0]\n",
    "        epoch_loss += loss * batch_size\n",
    "        num_samples += batch_size\n",
    "\n",
    "    avg_loss = epoch_loss / num_samples\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} — avg loss: {avg_loss:.4f}\")\n",
    "\n",
    "# после обучения — строим кривую потерь\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(range(1, num_epochs+1), losses, marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "f46975e326dff6ad",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 1/2:   0%|          | 0/1875 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1/2:   0%|          | 1/1875 [00:03<1:40:12,  3.21s/it]\u001B[A\n",
      "Epoch 1/2:   0%|          | 2/1875 [00:06<1:36:52,  3.10s/it]\u001B[A\n",
      "Epoch 1/2:   0%|          | 3/1875 [00:09<1:36:25,  3.09s/it]\u001B[A\n",
      "Epoch 1/2:   0%|          | 4/1875 [00:12<1:36:07,  3.08s/it]\u001B[A\n",
      "Epoch 1/2:   0%|          | 5/1875 [00:15<1:35:49,  3.07s/it]\u001B[A\n",
      "Epoch 1/2:   0%|          | 6/1875 [00:18<1:35:01,  3.05s/it]\u001B[A\n",
      "Epoch 1/2:   0%|          | 7/1875 [00:21<1:35:54,  3.08s/it]\u001B[A\n",
      "Epoch 1/2:   0%|          | 8/1875 [00:24<1:35:31,  3.07s/it]\u001B[A\n",
      "Epoch 1/2:   0%|          | 9/1875 [00:27<1:35:26,  3.07s/it]\u001B[A\n",
      "Epoch 1/2:   1%|          | 10/1875 [00:30<1:35:03,  3.06s/it]\u001B[A\n",
      "Epoch 1/2:   1%|          | 11/1875 [00:33<1:35:38,  3.08s/it]\u001B[A\n",
      "Epoch 1/2:   1%|          | 12/1875 [00:36<1:34:57,  3.06s/it]\u001B[A\n",
      "Epoch 1/2:   1%|          | 13/1875 [00:40<1:35:40,  3.08s/it]\u001B[A\n",
      "Epoch 1/2:   1%|          | 14/1875 [00:43<1:35:30,  3.08s/it]\u001B[A\n",
      "Epoch 1/2:   1%|          | 15/1875 [00:46<1:35:03,  3.07s/it]\u001B[A\n",
      "Epoch 1/2:   1%|          | 16/1875 [00:49<1:35:02,  3.07s/it]\u001B[A\n",
      "Epoch 1/2:   1%|          | 17/1875 [00:52<1:36:20,  3.11s/it]\u001B[A\n",
      "Epoch 1/2:   1%|          | 18/1875 [00:55<1:36:54,  3.13s/it]\u001B[A\n",
      "Epoch 1/2:   1%|          | 19/1875 [00:58<1:36:27,  3.12s/it]\u001B[A\n",
      "Epoch 1/2:   1%|          | 20/1875 [01:01<1:34:59,  3.07s/it]\u001B[A\n",
      "Epoch 1/2:   1%|          | 21/1875 [01:04<1:35:11,  3.08s/it]\u001B[A\n",
      "Epoch 1/2:   1%|          | 22/1875 [01:07<1:36:01,  3.11s/it]\u001B[A\n",
      "Epoch 1/2:   1%|          | 23/1875 [01:10<1:35:02,  3.08s/it]\u001B[A\n",
      "Epoch 1/2:   1%|▏         | 24/1875 [01:13<1:34:13,  3.05s/it]\u001B[A\n",
      "Epoch 1/2:   1%|▏         | 25/1875 [01:16<1:34:16,  3.06s/it]\u001B[A\n",
      "Epoch 1/2:   1%|▏         | 26/1875 [01:20<1:34:23,  3.06s/it]\u001B[A\n",
      "Epoch 1/2:   1%|▏         | 27/1875 [01:23<1:35:07,  3.09s/it]\u001B[A\n",
      "Epoch 1/2:   1%|▏         | 28/1875 [01:26<1:34:46,  3.08s/it]\u001B[A\n",
      "Epoch 1/2:   2%|▏         | 29/1875 [01:29<1:34:48,  3.08s/it]\u001B[A\n",
      "Epoch 1/2:   2%|▏         | 30/1875 [01:32<1:34:59,  3.09s/it]\u001B[A\n",
      "Epochs:   0%|          | 0/2 [01:33<?, ?it/s]                 \u001B[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[78], line 17\u001B[0m\n\u001B[1;32m     14\u001B[0m y_idx \u001B[38;5;241m=\u001B[39m y_batch\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mint\u001B[39m)\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[1;32m     15\u001B[0m y_onehot \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39meye(\u001B[38;5;241m10\u001B[39m)[y_idx]\u001B[38;5;241m.\u001B[39mT  \u001B[38;5;66;03m# shape (10, batch)\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m logits_batch \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_np\u001B[49m\u001B[43m)\u001B[49m       \u001B[38;5;66;03m# (batch, 10)\u001B[39;00m\n\u001B[1;32m     18\u001B[0m logits \u001B[38;5;241m=\u001B[39m logits_batch\u001B[38;5;241m.\u001B[39mT                  \u001B[38;5;66;03m# (10, batch)\u001B[39;00m\n\u001B[1;32m     19\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn\u001B[38;5;241m.\u001B[39mforward(logits, y_onehot)\n",
      "Cell \u001B[0;32mIn[75], line 16\u001B[0m, in \u001B[0;36mConvNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: NDArray) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m NDArray:\n\u001B[0;32m---> 16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[44], line 228\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    226\u001B[0m out \u001B[38;5;241m=\u001B[39m x\n\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m--> 228\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "Cell \u001B[0;32mIn[44], line 108\u001B[0m, in \u001B[0;36mConv2D.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    106\u001B[0m                 h0, w0 \u001B[38;5;241m=\u001B[39m stride\u001B[38;5;241m*\u001B[39mi, stride\u001B[38;5;241m*\u001B[39mj\n\u001B[1;32m    107\u001B[0m                 region \u001B[38;5;241m=\u001B[39m x_pad[n, :, h0:h0\u001B[38;5;241m+\u001B[39mkernel_size, w0:w0\u001B[38;5;241m+\u001B[39mkernel_size]\n\u001B[0;32m--> 108\u001B[0m                 out[n, out_c, i, j] \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mregion\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mkernel\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m bias\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/app/anaconda3/envs/dev/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:2466\u001B[0m, in \u001B[0;36msum\u001B[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001B[0m\n\u001B[1;32m   2463\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[1;32m   2464\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\u001B[0;32m-> 2466\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_wrapreduction\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2467\u001B[0m \u001B[43m    \u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msum\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2468\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhere\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwhere\u001B[49m\n\u001B[1;32m   2469\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/app/anaconda3/envs/dev/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:86\u001B[0m, in \u001B[0;36m_wrapreduction\u001B[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001B[0m\n\u001B[1;32m     83\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     84\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m reduction(axis\u001B[38;5;241m=\u001B[39maxis, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpasskwargs)\n\u001B[0;32m---> 86\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mufunc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpasskwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_classes = 10\n",
    "correct_per_class = np.zeros(num_classes, dtype=int)\n",
    "total_per_class   = np.zeros(num_classes, dtype=int)\n",
    "total_correct     = 0\n",
    "total_samples     = 0\n",
    "\n",
    "for X_batch, y_batch in testloader:\n",
    "    X = np.asarray(X_batch.numpy())\n",
    "    y_idx = y_batch.numpy().astype(int)\n",
    "    y = np.eye(10, dtype=np.float32)[y_idx]\n",
    "\n",
    "    logits = model.forward(X)\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "\n",
    "    total_samples += y_idx.shape[0]\n",
    "    total_correct += np.sum(preds == y_idx)\n",
    "\n",
    "    for pred, label in zip(preds, y_idx):\n",
    "        total_per_class[label]   += 1\n",
    "        correct_per_class[label] += (pred == label)\n",
    "\n",
    "overall_acc = total_correct / total_samples\n",
    "class_acc   = correct_per_class / total_per_class\n",
    "\n",
    "print(f\"Overall accuracy: {overall_acc*100:.2f}%\\n\")\n",
    "\n",
    "for cls in range(num_classes):\n",
    "    print(f\"Class {cls}: {class_acc[cls]*100:.2f}%  \" +\n",
    "          f\"({correct_per_class[cls]}/{total_per_class[cls]})\")\n"
   ],
   "id": "54ae089e1eed647e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
